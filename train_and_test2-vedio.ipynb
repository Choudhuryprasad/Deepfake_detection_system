{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b7917d6-fd75-47ec-a761-c40067eabec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\khadi\\anaconda3\\lib\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\khadi\\anaconda3\\lib\\site-packages (0.19.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\khadi\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\khadi\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\khadi\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\khadi\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\khadi\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\khadi\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\khadi\\anaconda3\\lib\\site-packages (from torch) (74.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\khadi\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\khadi\\anaconda3\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\khadi\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\khadi\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24b4b4a3-3ba1-4a51-a5c4-27b53bdcd57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7c63ea7-a519-426a-9766-7e7605f57440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Labels: ['Fake', 'Real']\n",
      "Train Set Size: 2800 images\n",
      "Validation Set Size: 600 images\n"
     ]
    }
   ],
   "source": [
    "# Check Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  \n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  \n",
    "])\n",
    "\n",
    "# Load Datasets\n",
    "train_dir = \"D:/Singularity101/Dataset_Preprocessed/train\"\n",
    "val_dir = \"D:/Singularity101/Dataset_Preprocessed/val\"\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Print dataset info\n",
    "print(f\"Class Labels: {train_dataset.classes}\")\n",
    "print(f\"Train Set Size: {len(train_dataset)} images\")\n",
    "print(f\"Validation Set Size: {len(val_dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b65315f4-a3e4-4f7a-a2aa-99ca8be3831c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNetLSTM(\n",
      "  (resnet_features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (lstm): LSTM(2048, 512, batch_first=True)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNetLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim=512, num_layers=1, num_classes=1):\n",
    "        super(ResNetLSTM, self).__init__()\n",
    "\n",
    "        # Load pre-trained ResNet50 (Fixed: Use recommended weights)\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "        # Remove last fully connected layer\n",
    "        self.resnet_features = nn.Sequential(*list(resnet.children())[:-2])  # Keep conv features\n",
    "\n",
    "        # Adaptive pooling to ensure fixed-size output\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  \n",
    "\n",
    "        # LSTM for sequence processing\n",
    "        self.lstm = nn.LSTM(input_size=2048, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, c, h, w = x.shape  # (Batch, TimeSteps, C, H, W)\n",
    "\n",
    "        # Reshape input: (Batch * TimeSteps, C, H, W) for batch processing\n",
    "        x = x.view(batch_size * seq_len, c, h, w)\n",
    "\n",
    "        # Extract features using ResNet (Faster than loop)\n",
    "        x = self.resnet_features(x)  # (Batch * TimeSteps, 2048, H', W')\n",
    "        x = self.avgpool(x)  # (Batch * TimeSteps, 2048, 1, 1)\n",
    "        x = x.view(batch_size, seq_len, -1)  # Reshape to (Batch, TimeSteps, 2048)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        # Take last LSTM output\n",
    "        output = self.fc(lstm_out[:, -1, :])  # (Batch, num_classes)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Check Model Architecture\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNetLSTM().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43292217-8425-4bf2-968f-dd42373146f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.tensor([1.0, 1.5]).to(device)  # Tune this value\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=5e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-5, max_lr=1e-3, step_size_up=5, mode=\"triangular2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2618e408-ba31-4edd-bf62-c6deea86d4ee",
   "metadata": {},
   "source": [
    "# BEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d442ba7-0b95-42ba-941d-2094823813e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.8324\n",
      "Epoch [2/50], Loss: 0.6983\n",
      "Epoch [3/50], Loss: 0.5385\n",
      "Epoch [4/50], Loss: 0.3956\n",
      "Epoch [5/50], Loss: 0.2331\n",
      "Epoch [6/50], Loss: 0.0935\n",
      "Epoch [7/50], Loss: 0.0435\n",
      "Epoch [8/50], Loss: 0.0237\n",
      "Epoch [9/50], Loss: 0.0200\n",
      "Epoch [10/50], Loss: 0.0105\n",
      "Epoch [11/50], Loss: 0.0070\n",
      "Epoch [12/50], Loss: 0.0074\n",
      "Epoch [13/50], Loss: 0.0022\n",
      "Epoch [14/50], Loss: 0.0013\n",
      "Epoch [15/50], Loss: 0.0045\n",
      "Epoch [16/50], Loss: 0.0028\n",
      "Epoch [17/50], Loss: 0.0071\n",
      "Epoch [18/50], Loss: 0.0016\n",
      "Epoch [19/50], Loss: 0.0085\n",
      "Epoch [20/50], Loss: 0.0009\n",
      "Epoch [21/50], Loss: 0.0007\n",
      "Epoch [22/50], Loss: 0.0049\n",
      "Epoch [23/50], Loss: 0.0021\n",
      "Epoch [24/50], Loss: 0.0005\n",
      "Epoch [25/50], Loss: 0.0008\n",
      "Epoch [26/50], Loss: 0.0031\n",
      "Epoch [27/50], Loss: 0.0003\n",
      "Epoch [28/50], Loss: 0.0058\n",
      "Epoch [29/50], Loss: 0.0009\n",
      "Epoch [30/50], Loss: 0.0003\n",
      "Epoch [31/50], Loss: 0.0056\n",
      "Epoch [32/50], Loss: 0.0002\n",
      "Epoch [33/50], Loss: 0.0002\n",
      "Epoch [34/50], Loss: 0.0014\n",
      "Epoch [35/50], Loss: 0.0087\n",
      "Epoch [36/50], Loss: 0.0001\n",
      "Epoch [37/50], Loss: 0.0001\n",
      "Epoch [38/50], Loss: 0.0001\n",
      "Epoch [39/50], Loss: 0.0001\n",
      "Epoch [40/50], Loss: 0.0006\n",
      "Epoch [41/50], Loss: 0.0040\n",
      "Epoch [42/50], Loss: 0.0001\n",
      "Epoch [43/50], Loss: 0.0001\n",
      "Epoch [44/50], Loss: 0.0001\n",
      "Epoch [45/50], Loss: 0.0020\n",
      "Epoch [46/50], Loss: 0.0004\n",
      "Epoch [47/50], Loss: 0.0060\n",
      "Epoch [48/50], Loss: 0.0003\n",
      "Epoch [49/50], Loss: 0.0009\n",
      "Epoch [50/50], Loss: 0.0002\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50  # Adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        images = images.unsqueeze(1)  # Add sequence dimension\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels.float().unsqueeze(1))  # Ensure labels match output shape\n",
    "        loss.backward()\n",
    "\n",
    "        # Apply gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a496788-ecf8-40ce-b4e6-0866c60da855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(labels[:10])  # Check first 10 labels\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8791f1e9-4a23-4030-a899-989b0d5bf1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  9.2657],\n",
      "        [-11.4235],\n",
      "        [  7.4926],\n",
      "        [  9.6913],\n",
      "        [  5.7407],\n",
      "        [  8.3859],\n",
      "        [  6.3817],\n",
      "        [  9.6917],\n",
      "        [  9.1599],\n",
      "        [  4.2023]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    images, labels = images.to(device), labels.to(device).float()\n",
    "    images = images.unsqueeze(1)  # Ensure shape is (Batch, TimeSteps=1, C, H, W)\n",
    "    \n",
    "    outputs = model(images)\n",
    "    print(outputs[:10])  # Check first 10 outputs\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bae84b60-e4d8-4387-b15f-e268d82635b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1562119573354721\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.abs().mean().item())  # Check weight magnitude\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47bc9864-b957-410a-bc37-ae1054441082",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"resnet_lstm_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "698a8d0b-1cec-48dd-9a9f-e2671c31b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "def test_model(model, data_loader, device):\n",
    "    model.eval()  \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            images = images.unsqueeze(1)  \n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).cpu().numpy().astype(int)\n",
    "\n",
    "            all_preds.extend(preds.flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    return np.array(all_labels), np.array(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76e966c2-36d7-4180-ad0c-dd9b042e3918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6683\n",
      "Confusion Matrix:\n",
      " [[200 100]\n",
      " [ 99 201]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.67      0.67      0.67       300\n",
      "        Real       0.67      0.67      0.67       300\n",
      "\n",
      "    accuracy                           0.67       600\n",
      "   macro avg       0.67      0.67      0.67       600\n",
      "weighted avg       0.67      0.67      0.67       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "true_labels, pred_labels = test_model(model, val_loader, device)\n",
    "\n",
    "# Compute Metrics\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
    "class_report = classification_report(true_labels, pred_labels, target_names=['Fake', 'Real'])\n",
    "\n",
    "# Print Results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdc9bd3a-c44c-41c0-a3f2-5cd0e99e2f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: Fake (Confidence: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load image\n",
    "image_path = \"frame_006.jpg\"\n",
    "image = Image.open(image_path).convert(\"L\")  # Convert to grayscale\n",
    "\n",
    "# Define transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Preprocess image\n",
    "image = transform(image).unsqueeze(0).unsqueeze(0).to(device)  # Add batch & sequence dim\n",
    "\n",
    "# Get prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    prediction = torch.sigmoid(output).item()\n",
    "    label = \"Real\" if prediction > 0.5 else \"Fake\"\n",
    "\n",
    "print(f\"Predicted Label: {label} (Confidence: {prediction:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f8acd4b-863d-4d59-b85d-08e91feb8669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6683\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.67      0.67      0.67       300\n",
      "        Real       0.67      0.67      0.67       300\n",
      "\n",
      "    accuracy                           0.67       600\n",
      "   macro avg       0.67      0.67      0.67       600\n",
      "weighted avg       0.67      0.67      0.67       600\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "def plot_confusion_matrix(true_labels, pred_labels, class_names):\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    \n",
    "    # Plot confusion matrix as an image\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    \n",
    "    # Save the image\n",
    "    plt.savefig(\"confusion_matrix.png\")\n",
    "    plt.show()\n",
    "\n",
    "def test_and_plot_conf_mat(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            images = images.unsqueeze(1)  # Add sequence dimension\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = torch.sigmoid(outputs)  # Convert logits to probabilities\n",
    "            preds = (preds > 0.5).int()  # Convert probabilities to binary labels\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy().flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    true_labels, pred_labels = np.array(all_labels), np.array(all_preds)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, pred_labels, target_names=['Fake', 'Real']))\n",
    "\n",
    "    # Plot and save confusion matrix\n",
    "    plot_confusion_matrix(true_labels, pred_labels, class_names=['Fake', 'Real'])\n",
    "\n",
    "# Call the function to test and visualize\n",
    "test_and_plot_conf_mat(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa0105-de8e-4837-b335-9ad04e22f4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
