{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "166ee6ef-0558-4417-a337-52c03b655059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Labels: ['fake', 'real']\n",
      "Train Set Size: 2880 images\n",
      "Validation Set Size: 360 images\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations (convert grayscale to 3-channel RGB)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to 3-channel\n",
    "    transforms.Resize((224, 224)),  # Resize to match ResNet input\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize for 3 channels\n",
    "])\n",
    "\n",
    "# Dataset paths (already preprocessed)\n",
    "train_dir = \"D:/Singularity101/processed-dataset-new-n/preprocessed_train\"\n",
    "val_dir = \"D:/Singularity101/processed-dataset-new-n/preprocessed_val\"\n",
    "\n",
    "# Load dataset using ImageFolder with transformations\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Print dataset info\n",
    "print(f\"Class Labels: {train_dataset.classes}\")\n",
    "print(f\"Train Set Size: {len(train_dataset)} images\")\n",
    "print(f\"Validation Set Size: {len(val_dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7410b271-7e6e-4e42-8169-a6aafd5d6e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load ResNet-50 with pre-trained weights\n",
    "model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Modify final layer for binary classification\n",
    "model.fc = nn.Linear(model.fc.in_features, 1)  # Output = 1 (real/fake)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model summary (optional)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49d2ee10-21ec-417f-a460-f03654303c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08671626-601d-44fa-b333-83a58f084cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device).float().unsqueeze(1)  # Ensure label shape\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5beea705-31e3-4626-9e64-96f3760ad6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 0.2571\n",
      "Epoch [2/25], Loss: 0.0608\n",
      "Epoch [3/25], Loss: 0.0427\n",
      "Epoch [4/25], Loss: 0.0342\n",
      "Epoch [5/25], Loss: 0.0206\n",
      "Epoch [6/25], Loss: 0.0339\n",
      "Epoch [7/25], Loss: 0.0344\n",
      "Epoch [8/25], Loss: 0.0168\n",
      "Epoch [9/25], Loss: 0.0302\n",
      "Epoch [10/25], Loss: 0.0233\n",
      "Epoch [11/25], Loss: 0.0164\n",
      "Epoch [12/25], Loss: 0.0232\n",
      "Epoch [13/25], Loss: 0.0154\n",
      "Epoch [14/25], Loss: 0.0214\n",
      "Epoch [15/25], Loss: 0.0437\n",
      "Epoch [16/25], Loss: 0.0242\n",
      "Epoch [17/25], Loss: 0.0114\n",
      "Epoch [18/25], Loss: 0.0044\n",
      "Epoch [19/25], Loss: 0.0034\n",
      "Epoch [20/25], Loss: 0.0031\n",
      "Epoch [21/25], Loss: 0.0020\n",
      "Epoch [22/25], Loss: 0.0011\n",
      "Epoch [23/25], Loss: 0.0037\n",
      "Epoch [24/25], Loss: 0.0024\n",
      "Epoch [25/25], Loss: 0.0029\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1054eb9-50b6-470c-b04e-cf7568f877bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 99.44%\n",
      "Validation Loss: 0.0135\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.BCEWithLogitsLoss()  # Because output layer has no activation\n",
    "\n",
    "# Initialize variables\n",
    "correct = 0\n",
    "total = 0\n",
    "val_loss = 0.0\n",
    "\n",
    "# No gradient calculation for evaluation\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device).float().unsqueeze(1)  # Ensure correct shape\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        # Convert logits to binary predictions (0 or 1)\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "# Compute accuracy and average loss\n",
    "accuracy = 100 * correct / total\n",
    "avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47c091cd-2024-4c1f-b74e-bf41d4c9b74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9890, Recall: 1.0000, F1-score: 0.9945\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "        outputs = model(images)\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        \n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ced6cfae-6914-42b2-be3c-272d0a2f4d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbwAAAGHCAYAAADLDeexAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABHfklEQVR4nO3deVwUR/o/8M9wDYcwCig4CgiKiSgqohKJBrxjPGJM4m1QWeOtiIJLXAU1grKJEu94geKBbiJGo3E10WgMGsUrXtEYUTxg8UAIiMNVvz/8Od+MoGFgmAH6886rX5uurup+mmV5tqqrumVCCAEiIqIazsjQARAREekDEx4REUkCEx4REUkCEx4REUkCEx4REUkCEx4REUkCEx4REUkCEx4REUkCEx4REUkCEx5p7ddff8WoUaPg6uoKc3Nz1KpVC23atEF0dDQePXpUqdc+e/Ys/Pz8oFAoIJPJEBMTo/NryGQyRERE6Py8fycuLg4ymQwymQw//vhjieNCCDRp0gQymQz+/v7lusbKlSsRFxenVZsff/zxpTERVScmhg6Aqpe1a9diwoQJeO211xASEgIPDw8UFBQgOTkZq1evxvHjx5GYmFhp1x89ejRyc3ORkJCAOnXqoFGjRjq/xvHjx9GwYUOdn7esrK2tsX79+hJJ7ciRI/jjjz9gbW1d7nOvXLkS9vb2GDlyZJnbtGnTBsePH4eHh0e5r0tUFTDhUZkdP34c48ePR/fu3bFr1y7I5XL1se7du2P69OnYv39/pcZw8eJFjBkzBr169aq0a7zxxhuVdu6yGDRoELZs2YIVK1bAxsZGXb5+/Xp06NAB2dnZeomjoKAAMpkMNjY2Bv+ZEOkChzSpzCIjIyGTybBmzRqNZPecmZkZ+vXrp94vLi5GdHQ0Xn/9dcjlctSrVw8fffQR7ty5o9HO398fLVq0wKlTp9CpUydYWlrCzc0NCxcuRHFxMYD/G+4rLCzEqlWr1EN/ABAREaH+97963ubmzZvqskOHDsHf3x92dnawsLCAs7Mz3n//fTx58kRdp7QhzYsXL+Ldd99FnTp1YG5ujtatW2Pjxo0adZ4P/W3btg2zZs2CUqmEjY0NunXrhqtXr5bthwxgyJAhAIBt27apy7KysvD1119j9OjRpbaZO3cufHx8YGtrCxsbG7Rp0wbr16/HX98N36hRI1y6dAlHjhxR//ye95Cfxx4fH4/p06ejQYMGkMvluH79eokhzQcPHsDJyQm+vr4oKChQn//y5cuwsrLCiBEjynyvRPrEhEdlUlRUhEOHDsHb2xtOTk5lajN+/HjMnDkT3bt3x+7duzF//nzs378fvr6+ePDggUbd9PR0DBs2DMOHD8fu3bvRq1cvhIWFYfPmzQCA3r174/jx4wCADz74AMePH1fvl9XNmzfRu3dvmJmZYcOGDdi/fz8WLlwIKysr5Ofnv7Td1atX4evri0uXLmHp0qXYuXMnPDw8MHLkSERHR5eo/8knn+DWrVtYt24d1qxZg99//x19+/ZFUVFRmeK0sbHBBx98gA0bNqjLtm3bBiMjIwwaNOil9zZ27Fjs2LEDO3fuxIABAzB58mTMnz9fXScxMRFubm7w8vJS//xeHH4OCwtDamoqVq9ejT179qBevXolrmVvb4+EhAScOnUKM2fOBAA8efIEH374IZydnbF69eoy3SeR3gmiMkhPTxcAxODBg8tU/8qVKwKAmDBhgkb5L7/8IgCITz75RF3m5+cnAIhffvlFo66Hh4fo2bOnRhkAMXHiRI2y8PBwUdqvcmxsrAAgUlJShBBCfPXVVwKAOHfu3CtjByDCw8PV+4MHDxZyuVykpqZq1OvVq5ewtLQUjx8/FkIIcfjwYQFAvPPOOxr1duzYIQCI48ePv/K6z+M9deqU+lwXL14UQgjRrl07MXLkSCGEEM2bNxd+fn4vPU9RUZEoKCgQ8+bNE3Z2dqK4uFh97GVtn1/vrbfeeumxw4cPa5QvWrRIABCJiYkiICBAWFhYiF9//fWV90hkSOzhUaU4fPgwAJSYHNG+fXs0a9YMP/zwg0a5o6Mj2rdvr1HWsmVL3Lp1S2cxtW7dGmZmZvj444+xceNG3Lhxo0ztDh06hK5du5bo2Y4cORJPnjwp0dP867Au8Ow+AGh1L35+fmjcuDE2bNiACxcu4NSpUy8dznweY7du3aBQKGBsbAxTU1PMmTMHDx8+REZGRpmv+/7775e5bkhICHr37o0hQ4Zg48aNWLZsGTw9PcvcnkjfmPCoTOzt7WFpaYmUlJQy1X/48CEAoH79+iWOKZVK9fHn7OzsStSTy+XIy8srR7Sla9y4Mb7//nvUq1cPEydOROPGjdG4cWN88cUXr2z38OHDl97H8+N/9eK9PH/eqc29yGQyjBo1Cps3b8bq1avRtGlTdOrUqdS6J0+eRI8ePQA8m0X7888/49SpU5g1a5bW1y3tPl8V48iRI/H06VM4Ojry2R1VeUx4VCbGxsbo2rUrTp8+XWLSSWme/9FPS0srcezevXuwt7fXWWzm5uYAAJVKpVH+4nNCAOjUqRP27NmDrKwsnDhxAh06dEBQUBASEhJeen47O7uX3gcAnd7LX40cORIPHjzA6tWrMWrUqJfWS0hIgKmpKb799lsMHDgQvr6+aNu2bbmuWdrkn5dJS0vDxIkT0bp1azx8+BAzZswo1zWJ9IUJj8osLCwMQgiMGTOm1EkeBQUF2LNnDwCgS5cuAKCedPLcqVOncOXKFXTt2lVncT2fafjrr79qlD+PpTTGxsbw8fHBihUrAABnzpx5ad2uXbvi0KFD6gT33KZNm2BpaVlpU/YbNGiAkJAQ9O3bFwEBAS+tJ5PJYGJiAmNjY3VZXl4e4uPjS9TVVa+5qKgIQ4YMgUwmw3fffYeoqCgsW7YMO3furPC5iSoL1+FRmXXo0AGrVq3ChAkT4O3tjfHjx6N58+YoKCjA2bNnsWbNGrRo0QJ9+/bFa6+9ho8//hjLli2DkZERevXqhZs3b2L27NlwcnLCtGnTdBbXO++8A1tbWwQGBmLevHkwMTFBXFwcbt++rVFv9erVOHToEHr37g1nZ2c8ffpUPROyW7duLz1/eHg4vv32W3Tu3Blz5syBra0ttmzZgr179yI6OhoKhUJn9/KihQsX/m2d3r17Y/HixRg6dCg+/vhjPHz4EJ999lmpS0c8PT2RkJCA7du3w83NDebm5uV67hYeHo6ffvoJBw4cgKOjI6ZPn44jR44gMDAQXl5ecHV11fqcRJXO0LNmqPo5d+6cCAgIEM7OzsLMzExYWVkJLy8vMWfOHJGRkaGuV1RUJBYtWiSaNm0qTE1Nhb29vRg+fLi4ffu2xvn8/PxE8+bNS1wnICBAuLi4aJShlFmaQghx8uRJ4evrK6ysrESDBg1EeHi4WLduncYszePHj4v33ntPuLi4CLlcLuzs7ISfn5/YvXt3iWv8dZamEEJcuHBB9O3bVygUCmFmZiZatWolYmNjNeo8n834n//8R6M8JSVFAChR/0V/naX5KqXNtNywYYN47bXXhFwuF25ubiIqKkqsX79e4/6FEOLmzZuiR48ewtraWgBQ/3xfFvtfjz2fpXngwAFhZGRU4mf08OFD4ezsLNq1aydUKtUr74HIEGRC/GVlKhERUQ3FZ3hERCQJTHhERCQJTHhERCQJTHhERFRpjh49ir59+0KpVEImk2HXrl0ax3NycjBp0iQ0bNgQFhYWaNasGVatWqVRR6VSYfLkybC3t4eVlRX69etXpvXAL2LCIyKiSpObm4tWrVph+fLlpR6fNm0a9u/fj82bN+PKlSuYNm0aJk+ejG+++UZdJygoCImJiUhISMCxY8eQk5ODPn36lPmF7M9xliYREemFTCZDYmIi+vfvry5r0aIFBg0ahNmzZ6vLvL298c4772D+/PnIyspC3bp1ER8fr/5ayL179+Dk5IR9+/ahZ8+eZb4+e3hERKQVlUqF7Oxsje3FV/uVVceOHbF7927cvXsXQggcPnwY165dUyey06dPo6CgQP2+WODZe2xbtGiBpKQkra5VI9+0YuE91dAhkEQ8PBFj6BBIIixNy/6e07Kw8JpU7rYz37XH3LlzNcrCw8NLfDi5LJYuXYoxY8agYcOGMDExgZGREdatW4eOHTsCePatTDMzM9SpU0ejnYODA9LT07W6Vo1MeERE9Ddk5R/gCwsLQ3BwsEZZaa+yK4ulS5fixIkT2L17N1xcXHD06FFMmDAB9evXf+Ur/4QQWr3sHGDCIyKSJi2TxV/J5fJyJ7i/ysvLwyeffILExET07t0bwLPvR547dw6fffYZunXrBkdHR+Tn5yMzM1Ojl5eRkQFfX1+trsdneEREUiQzKv+mIwUFBSgoKICRkeY5jY2NUVxcDODZBBZTU1McPHhQfTwtLQ0XL17UOuGxh0dERJUmJycH169fV++npKTg3LlzsLW1hbOzM/z8/BASEgILCwu4uLjgyJEj2LRpExYvXgwAUCgUCAwMxPTp02FnZwdbW1vMmDEDnp6erxzyLA0THhGRFFVgSFMbycnJ6Ny5s3r/+bO/gIAAxMXFISEhAWFhYRg2bBgePXoEFxcXLFiwAOPGjVO3WbJkCUxMTDBw4EDk5eWha9euiIuL0/gGZFnUyHV4nKVJ+sJZmqQvOp+l2b78X6jPO/mZDiPRH/bwiIikSE89vKqECY+ISIp0OPmkumDCIyKSIgn28KSX4omISJLYwyMikiIOaRIRkSRIcEiTCY+ISIrYwyMiIklgD4+IiCRBgj086d0xERFJEnt4RERSJMEeHhMeEZEUGfEZHhERSQF7eEREJAmcpUlERJIgwR6e9O6YiIgkiT08IiIp4pAmERFJggSHNJnwiIikiD08IiKSBPbwiIhIEiTYw5NeiiciIkliD4+ISIokOKQpvTsmIqJnQ5rl3bRw9OhR9O3bF0qlEjKZDLt27SpR58qVK+jXrx8UCgWsra3xxhtvIDU1VX1cpVJh8uTJsLe3h5WVFfr164c7d+5ofctMeEREUiQzKv+mhdzcXLRq1QrLly8v9fgff/yBjh074vXXX8ePP/6I8+fPY/bs2TA3N1fXCQoKQmJiIhISEnDs2DHk5OSgT58+KCoq0u6WhRBCqxbVgIX3VEOHQBLx8ESMoUMgibA01e0kE4u+K8vdNm/PhHK1k8lkSExMRP/+/dVlgwcPhqmpKeLj40ttk5WVhbp16yI+Ph6DBg0CANy7dw9OTk7Yt28fevbsWebrs4dHRCRFFRjSVKlUyM7O1thUKpXWIRQXF2Pv3r1o2rQpevbsiXr16sHHx0dj2PP06dMoKChAjx491GVKpRItWrRAUlKSVtdjwiMiIq1ERUVBoVBobFFRUVqfJyMjAzk5OVi4cCHefvttHDhwAO+99x4GDBiAI0eOAADS09NhZmaGOnXqaLR1cHBAenq6VtfjLE0iIimqwCzNsLAwBAcHa5TJ5XKtz1NcXAwAePfddzFt2jQAQOvWrZGUlITVq1fDz8/vpW2FEJBpOYGGPTwiIimqwJCmXC6HjY2NxlaehGdvbw8TExN4eHholDdr1kw9S9PR0RH5+fnIzMzUqJORkQEHBwetrseER0QkRXqapfkqZmZmaNeuHa5evapRfu3aNbi4uAAAvL29YWpqioMHD6qPp6Wl4eLFi/D19dXqehzSJCKSIj29WiwnJwfXr19X76ekpODcuXOwtbWFs7MzQkJCMGjQILz11lvo3Lkz9u/fjz179uDHH38EACgUCgQGBmL69Omws7ODra0tZsyYAU9PT3Tr1k2rWJjwiIgkSNvnX+WVnJyMzp07q/efP/sLCAhAXFwc3nvvPaxevRpRUVGYMmUKXnvtNXz99dfo2LGjus2SJUtgYmKCgQMHIi8vD127dkVcXByMjY21ioXr8IgqgOvwSF90vQ7P8v0N5W775OvROoxEf9jDIyKSIH318KoSJjwiIimSXr5jwiMikiL28IiISBKY8IiISBKkmPC48JyIiCSBPTwiIgmSYg+PCY+ISIqkl++Y8IiIpIg9PCIikgQmPCIikgQpJjzO0iQiIklgD4+ISIKk2MNjwiMikiLp5TsmPCIiKWIPj4iIJIEJj4iIJEGKCY+zNImISBLYwyMikiLpdfCY8IiIpEiKQ5pMeEREEsSER0REksCER0REkiDFhMdZmkREVGmOHj2Kvn37QqlUQiaTYdeuXS+tO3bsWMhkMsTExGiUq1QqTJ48Gfb29rCyskK/fv1w584drWOpMgnvp59+wvDhw9GhQwfcvXsXABAfH49jx44ZODIiohpIVoFNC7m5uWjVqhWWL1/+ynq7du3CL7/8AqVSWeJYUFAQEhMTkZCQgGPHjiEnJwd9+vRBUVGRVrFUiYT39ddfo2fPnrCwsMDZs2ehUqkAAH/++SciIyMNHB0RUc0jk8nKvWmjV69e+PTTTzFgwICX1rl79y4mTZqELVu2wNTUVONYVlYW1q9fj88//xzdunWDl5cXNm/ejAsXLuD777/XKpYqkfA+/fRTrF69GmvXrtW4WV9fX5w5c8aAkRER1UwVSXgqlQrZ2dka2/OOiraKi4sxYsQIhISEoHnz5iWOnz59GgUFBejRo4e6TKlUokWLFkhKStLqWlUi4V29ehVvvfVWiXIbGxs8fvxY/wEREdVwFUl4UVFRUCgUGltUVFS54li0aBFMTEwwZcqUUo+np6fDzMwMderU0Sh3cHBAenq6VteqErM069evj+vXr6NRo0Ya5ceOHYObm5thgiIiolKFhYUhODhYo0wul2t9ntOnT+OLL77AmTNntB4qFUJo3aZK9PDGjh2LqVOn4pdffoFMJsO9e/ewZcsWzJgxAxMmTDB0eERENU8FJq3I5XLY2NhobOVJeD/99BMyMjLg7OwMExMTmJiY4NatW5g+fbq6A+To6Ij8/HxkZmZqtM3IyICDg4NW16sSPbzQ0FBkZWWhc+fOePr0Kd566y3I5XLMmDEDkyZNMnR41dabXo0x7aMuaNPMCfXrKjBw+jrs+fGC+nje6S9KbfdJzDdYEn8IAOBgZ43Iqe+ii89rsLaS49qtDPx7w0Ek/nBeL/dANcP6tV/i0PcHcTPlBuTm5mjV2gtTp01HI1eO4BhKVViHN2LECHTr1k2jrGfPnhgxYgRGjRoFAPD29oapqSkOHjyIgQMHAgDS0tJw8eJFREdHa3W9KpHw8vPzsWDBAsyaNQuXL19GcXExPDw8UKtWLTx48AD29vaGDrFasrIww4VrdxG/+xckfBZY4nijHv/S2O/h64HVcwYj8dD/JbP180ZAUcscHwavxYPHuRj0tjfio0bizRGf4fzVu5V+D1QznEk+hUFDhqJ5C08UFhZhxdIlGP/xP7Dzm29hYWlp6PAkSV8JLycnB9evX1fvp6Sk4Ny5c7C1tYWzszPs7Ow06puamsLR0RGvvfYaAEChUCAwMBDTp0+HnZ0dbG1tMWPGDHh6epZIln+nSiS8gQMHYufOnbC0tETbtm3V5f/73//QtWtXXLx40YDRVV8Hkq7gQNKVlx7/38M/Nfb7+rfAkeTruHn3obrMp2UjTInageRLqQCAResPYPJQf7R+3YkJj8psxZfrNPYjPo1C17d8cfnyJXi3bWegqKRNXwkvOTkZnTt3Vu8/f/YXEBCAuLi4Mp1jyZIlMDExwcCBA5GXl4euXbsiLi4OxsbGWsVSJRJeWloaAgMDERsbq1HWpUuXUqepku7Vs7XG2x2bY0z4Fo3ypHM38EGPNth/7DIe/5mHD7q3htzMBEdP/26gSKkmyMl59n+2FAqFgSORLn0lPH9/fwghylz/5s2bJcrMzc2xbNkyLFu2rEKxVIlJK/v27cPJkycxbdo0AM8WIfr7+8PT0xM7duwwcHTSMLxPO/yZ+xS7Dmk+mxsRFgcTYyPcOxyFrBOfY9msQRg0Yz1S7jx8yZmIXk0Igc+jF8KrjTeauDc1dDgkIVWih2dnZ4f//ve/6NixIwBg7969aNOmDbZs2QIjo1fnZJVKVWLBoyguhMyoStxatfHRu29g+3enocov1CiPGN8bdWws0GvcCjx8nIO+/i2xZdFIdPvHUly6nmagaKk6W7hgPn6/dhWxm7YaOhRpM/ycFb2rEj08AGjYsCEOHjyIrVu3on379ti2bVuZxmdLWwBZmJ6sh4hrjjdbu+G1Rg6I3XVco9y1oR3GD34LY+duw4+nruHC7/cQuXY/zly+jbEfdjJQtFSdLYycjyOHD2Hthk1wcHQ0dDiSpq9Xi1UlBusG1alTp9Qf3JMnT7Bnzx6NmTuPHj166XlKWwBZzy9Md4FKQED/N3D6ciou/H5Po9zS3AwAUFysOf5eVFwMI6Pq+0tP+ieEwKLI+Tj0w/dYG7sJDRo2NHRIkledE1d5GSzhvfj5h/KSy+UlFjxyOPMZKwszNHaqq95vpLRDy6YNkJn9BLfTny3itLaSY0C31vjnkm9KtL9683+4nnofy2cNRFjMN3iYlYt+/i3R1ec1DAhaq7f7oOov6tN5+G7ft1iydAWsrKzw4MF9AECtWtYwNzc3cHTSJMF8B5nQZvpMNWHhPdXQIVQJnbyb4MCaySXK4/f8go8jnj0/Gf1eB/x7xgC49pyN7JynJeo2dqqLTyf3RYfWbqhlaYY/bj9ATPwhbNvHYWMAeHgixtAhVAteLV4vtXzup5Ho1//lb9Gn/2NpqtsM5R6yv9xtf//32zqMRH+qXMLLy8tDQUGBRpmNjY1W52DCI31hwiN9YcKruCoxaSU3NxeTJk1CvXr1UKtWLdSpU0djIyIi3ZLJyr9VV1Ui4YWGhuLQoUNYuXIl5HI51q1bh7lz50KpVGLTpk2GDo+IqMbhLE0D2bNnDzZt2gR/f3+MHj0anTp1QpMmTeDi4oItW7Zg2LBhhg6RiKhGqcZ5q9yqRA/v0aNHcHV1BfDsed3zZQgdO3bE0aNHDRkaEVGNZGQkK/dWXVWJhOfm5qZ+f5qHh4f6dWJ79uxB7dq1DRcYEVENxWd4enbjxg0UFxdj1KhROH/+2Tscw8LC1M/ypk2bhpCQEEOGSERENYRBn+G5u7sjLS1N/dLoQYMGYenSpfjtt9+QnJyMxo0bo1WrVoYMkYioRqrOk0/Ky6A9vBeXAO7btw+5ublwdnbGgAEDmOyIiCqJFIc0q8QsTSIi0i8p9vAMmvBKW9Mhxf8SiIj0TYp/aw2a8IQQGDlypPrlz0+fPsW4ceNgZWWlUW/nzp2GCI+IqMaSYL4zbMILCAjQ2B8+fLiBIiEioprOoAkvNjbWkJcnIpIsDmkSEZEkSDDfMeEREUkRe3hERCQJEsx3VeNdmkREpF/6+jzQ0aNH0bdvXyiVSshkMuzatUt9rKCgADNnzoSnpyesrKygVCrx0Ucf4d69exrnUKlUmDx5Muzt7WFlZYV+/frhzp07Wt8zEx4REVWa3NxctGrVCsuXLy9x7MmTJzhz5gxmz56NM2fOYOfOnbh27Rr69eunUS8oKAiJiYlISEjAsWPHkJOTgz59+qCoqEirWDikSUQkQfoa0uzVqxd69epV6jGFQoGDBw9qlC1btgzt27dHamoqnJ2dkZWVhfXr1yM+Ph7dunUDAGzevBlOTk74/vvv0bNnzzLHwh4eEZEEVWRIU6VSITs7W2NTqVQ6iSsrKwsymUz9abjTp0+joKAAPXr0UNdRKpVo0aIFkpKStDo3Ex4RkQRV5OXRUVFRUCgUGltUVFSFY3r69Cn++c9/YujQobCxsQEApKenw8zMDHXq1NGo6+DggPT0dK3OzyFNIiIJqsiyhLCwMAQHB2uUPX9FZHkVFBRg8ODBKC4uxsqVK/+2vhBC63tgwiMikqCKPMOTy+UVTnB/VVBQgIEDByIlJQWHDh1S9+4AwNHREfn5+cjMzNTo5WVkZMDX11er63BIk4iIDOZ5svv999/x/fffw87OTuO4t7c3TE1NNSa3pKWl4eLFi1onPPbwiIgkSF9vWsnJycH169fV+ykpKTh37hxsbW2hVCrxwQcf4MyZM/j2229RVFSkfi5na2sLMzMzKBQKBAYGYvr06bCzs4OtrS1mzJgBT09P9azNsmLCIyKSIH0tS0hOTkbnzp3V+8+f/QUEBCAiIgK7d+8GALRu3Vqj3eHDh+Hv7w8AWLJkCUxMTDBw4EDk5eWha9euiIuLg7GxsVaxyIQQovy3UjVZeE81dAgkEQ9PxBg6BJIIS1PdZqhOnx8rd9ufpnfUYST6wx4eEZEE8eXRREQkCRLMd5ylSURE0sAeHhGRBHFIk4iIJEGC+Y4Jj4hIitjDIyIiSZBgvmPCIyKSIiMJZjzO0iQiIklgD4+ISIIk2MFjwiMikiJOWnmJ5y/3LIt+/fqVOxgiItIPI+nlu7IlvP79+5fpZDKZDEVFRRWJh4iI9IA9vJcoLi6u7DiIiEiPJJjvKjZL8+nTp7qKg4iIqFJpnfCKioowf/58NGjQALVq1cKNGzcAALNnz8b69et1HiAREemerAL/VFdaJ7wFCxYgLi4O0dHRMDMzU5d7enpi3bp1Og2OiIgqh5Gs/Ft1pXXC27RpE9asWYNhw4ZpfF69ZcuW+O2333QaHBERVQ6ZTFburbrSeh3e3bt30aRJkxLlxcXFKCgo0ElQRERUuapx3io3rXt4zZs3x08//VSi/D//+Q+8vLx0EhQREVUuI5ms3Ft1pXUPLzw8HCNGjMDdu3dRXFyMnTt34urVq9i0aRO+/fbbyoiRiIiowrTu4fXt2xfbt2/Hvn37IJPJMGfOHFy5cgV79uxB9+7dKyNGIiLSMZms/Ft1Va53afbs2RM9e/bUdSxERKQn1XnySXmV++XRycnJuHLlCmQyGZo1awZvb29dxkVERJVIgvlO+yHNO3fuoFOnTmjfvj2mTp2KKVOmoF27dujYsSNu375dGTESEZGO6WvSytGjR9G3b18olUrIZDLs2rVL47gQAhEREVAqlbCwsIC/vz8uXbqkUUelUmHy5Mmwt7eHlZUV+vXrhzt37mh/z9o2GD16NAoKCnDlyhU8evQIjx49wpUrVyCEQGBgoNYBEBGR/skqsGkjNzcXrVq1wvLly0s9Hh0djcWLF2P58uU4deoUHB0d0b17d/z555/qOkFBQUhMTERCQgKOHTuGnJwc9OnTR+uPFciEEEKbBhYWFkhKSiqxBOHMmTN48803kZeXp1UAlcHCe6qhQyCJeHgixtAhkERYmup2DHLwxrPlbpsQUL4laDKZDImJieov8AghoFQqERQUhJkzZwJ41ptzcHDAokWLMHbsWGRlZaFu3bqIj4/HoEGDAAD37t2Dk5MT9u3bp9V8Eq17eM7OzqUuMC8sLESDBg20PR0RERlARd60olKpkJ2drbGpVCqtY0hJSUF6ejp69OihLpPL5fDz80NSUhIA4PTp0ygoKNCoo1Qq0aJFC3WdstI64UVHR2Py5MlITk7G885hcnIypk6dis8++0zb0xERkQFU5F2aUVFRUCgUGltUVJTWMaSnpwMAHBwcNModHBzUx9LT02FmZoY6deq8tE5ZlWmWZp06dTSmsObm5sLHxwcmJs+aFxYWwsTEBKNHjy7zx2KJiMhwKrIsISwsDMHBwRplcrlcZ7EIIf42vrLUeVGZEl5MTIxWJyUioqqtIssS5HJ5hRLcc46OjgCe9eLq16+vLs/IyFD3+hwdHZGfn4/MzEyNXl5GRgZ8fX21ul6ZEl5AQIBWJyUioqqtKiw8d3V1haOjIw4ePKieCJmfn48jR45g0aJFAABvb2+Ympri4MGDGDhwIAAgLS0NFy9eRHR0tFbXK/fCcwDIy8srMYHFxsamIqckIqIaJCcnB9evX1fvp6Sk4Ny5c7C1tYWzszOCgoIQGRkJd3d3uLu7IzIyEpaWlhg6dCgAQKFQIDAwENOnT4ednR1sbW0xY8YMeHp6olu3blrFonXCy83NxcyZM7Fjxw48fPiwxHFt10UQEZH+6etDrsnJyejcubN6//mzv4CAAMTFxSE0NBR5eXmYMGECMjMz4ePjgwMHDsDa2lrdZsmSJTAxMcHAgQORl5eHrl27Ii4uTuObrGWh9Tq8iRMn4vDhw5g3bx4++ugjrFixAnfv3sWXX36JhQsXYtiwYVoFUBm4Do/0hevwSF90vQ5vVMKFcreNHeypw0j0R+se3p49e7Bp0yb4+/tj9OjR6NSpE5o0aQIXFxds2bKlSiQ8IiJ6NcM/wdM/rdfhPXr0CK6urgCePa979OgRAKBjx444evSobqMjIqJKIcUPwGqd8Nzc3HDz5k0AgIeHB3bs2AHgWc+vdu3auoyNiIhIZ7ROeKNGjcL58+cBPFt8uHLlSsjlckybNg0hISE6D5CIiHSPH4Atg2nTpqn/vXPnzvjtt9+QnJyMxo0bo1WrVjoNjoiIKkdVWIenb1r38F7k7OyMAQMGwNbWFqNHj9ZFTEREVMmk2MOrcMJ77tGjR9i4caOuTkdERJVIipNWKvSmFSIiqp6qcd4qN5318IiIiKoy9vCIiCRIipNWypzwBgwY8Mrjjx8/rmgsOpP5yxeGDoEkok67SYYOgSQi7+xynZ5PisN7ZU54CoXib49/9NFHFQ6IiIgqH3t4rxAbG1uZcRARkR7p62sJVQmf4RERSZAUE54Uh3GJiEiC2MMjIpIgPsMjIiJJkOKQJhMeEZEESbCDV75nePHx8XjzzTehVCpx69YtAEBMTAy++eYbnQZHRESVQ4rv0tQ64a1atQrBwcF455138PjxYxQVFQEAateujZiYGF3HR0RElcCoAlt1pXXsy5Ytw9q1azFr1iwYGxury9u2bYsLFy7oNDgiIiJd0foZXkpKCry8vEqUy+Vy5Obm6iQoIiKqXNV4ZLLctO7hubq64ty5cyXKv/vuO3h4eOgiJiIiqmR8hlcGISEhmDhxIrZv3w4hBE6ePIkFCxbgk08+QUhISGXESEREOqavL54XFhbiX//6F1xdXWFhYQE3NzfMmzcPxcXF6jpCCERERECpVMLCwgL+/v64dOmSju+4HEOao0aNQmFhIUJDQ/HkyRMMHToUDRo0wBdffIHBgwfrPEAiItI9fa3DW7RoEVavXo2NGzeiefPmSE5OxqhRo6BQKDB16lQAQHR0NBYvXoy4uDg0bdoUn376Kbp3746rV6/C2tpaZ7HIhBCivI0fPHiA4uJi1KtXT2cB6cLTQkNHQFLBzwORvuj680DzDl4vd9s53ZuUuW6fPn3g4OCA9evXq8vef/99WFpaIj4+HkIIKJVKBAUFYebMmQAAlUoFBwcHLFq0CGPHji13nC+q0AxTe3v7KpfsiIiocqlUKmRnZ2tsKpWq1LodO3bEDz/8gGvXrgEAzp8/j2PHjuGdd94B8GwiZHp6Onr06KFuI5fL4efnh6SkJJ3GrfWQpqur6yvfwXbjxo0KBURERJWvInNPoqKiMHfuXI2y8PBwRERElKg7c+ZMZGVl4fXXX4exsTGKioqwYMECDBkyBACQnp4OAHBwcNBo5+DgoH6xia5onfCCgoI09gsKCnD27Fns37+fk1aIiKqJijzDCw0LQ3BwsEaZXC4vte727duxefNmbN26Fc2bN8e5c+cQFBQEpVKJgIAAdb0XO1JCCJ2/4FrrhPf8IeOLVqxYgeTk5AoHRERElU+G8icTuVz+0gT3opCQEPzzn/9UT2r09PTErVu3EBUVhYCAADg6OgJ41tOrX7++ul1GRkaJXl9F6ewtMb169cLXX3+tq9MREVElMpKVf9PGkydPYGSkmWqMjY3VyxJcXV3h6OiIgwcPqo/n5+fjyJEj8PX1rfB9/pXOvpbw1VdfwdbWVlenIyKiSqSvZQl9+/bFggUL4OzsjObNm+Ps2bNYvHgxRo8eDeDZUGZQUBAiIyPh7u4Od3d3REZGwtLSEkOHDtVpLFonPC8vL41xVSEE0tPTcf/+faxcuVKnwRERUfW2bNkyzJ49GxMmTEBGRgaUSiXGjh2LOXPmqOuEhoYiLy8PEyZMQGZmJnx8fHDgwAGdrsEDyrEO78WZOUZGRqhbty78/f3x+uuv6zS48uI6PNIXrsMjfdH1Orx//1j+GfUh/m46jER/tOrhFRYWolGjRujZs6f6QSMREVU/UvziuVaTVkxMTDB+/PiXLjAkIqLqQV/v0qxKtJ6l6ePjg7Nnz1ZGLEREpCdS/FqC1pNWJkyYgOnTp+POnTvw9vaGlZWVxvGWLVvqLDgiIqocUhzSLHPCGz16NGJiYjBo0CAAwJQpU9THZDKZelV8UVGR7qMkIiKqoDInvI0bN2LhwoVISUmpzHiIiEgPqvHIZLmVOeE9X73g4uJSacEQEZF+GFXg1WLVlVbP8HT9Ik8iIjIMKf451yrhNW3a9G+T3qNHjyoUEBERVT5OWvkbc+fOhUKhqKxYiIhIT6rz8oLy0irhDR48mF84JyKiaqnMCY/P74iIag4p/knXepYmERFVfxzSfIXnH+sjIqLqT4L5TncfgCUioupD6xcp1wBMeEREEiTFeRlSTPJERCRB7OEREUmQ9Pp3THhERJLEWZpERCQJ0kt3THhERJIkwQ4eEx4RkRRxliYREVENxYRHRCRBRhXYtHX37l0MHz4cdnZ2sLS0ROvWrXH69Gn1cSEEIiIioFQqYWFhAX9/f1y6dKkit1cqJjwiIgmSyWTl3rSRmZmJN998E6ampvjuu+9w+fJlfP7556hdu7a6TnR0NBYvXozly5fj1KlTcHR0RPfu3fHnn3/q9J75DI+ISIL09QRv0aJFcHJyQmxsrLqsUaNG6n8XQiAmJgazZs3CgAEDAAAbN26Eg4MDtm7dirFjx+osFvbwiIgkqCI9PJVKhezsbI1NpVKVep3du3ejbdu2+PDDD1GvXj14eXlh7dq16uMpKSlIT09Hjx491GVyuRx+fn5ISkrS6T0z4RERSVBFnuFFRUVBoVBobFFRUaVe58aNG1i1ahXc3d3x3//+F+PGjcOUKVOwadMmAEB6ejoAwMHBQaOdg4OD+piucEiTiIi0EhYWhuDgYI0yuVxeat3i4mK0bdsWkZGRAAAvLy9cunQJq1atwkcffaSu9+KzQSGEzpdOsIdHRCRBFRnSlMvlsLGx0dhelvDq168PDw8PjbJmzZohNTUVAODo6AgAJXpzGRkZJXp9FcWER0QkQbIKbNp48803cfXqVY2ya9euwcXFBQDg6uoKR0dHHDx4UH08Pz8fR44cga+vr/Y39goc0iQikiB9vWhl2rRp8PX1RWRkJAYOHIiTJ09izZo1WLNmzf+PQ4agoCBERkbC3d0d7u7uiIyMhKWlJYYOHarTWJjwiIgkyEhPCxPatWuHxMREhIWFYd68eXB1dUVMTAyGDRumrhMaGoq8vDxMmDABmZmZ8PHxwYEDB2Btba3TWGRCCKHTM1YBTwsNHQFJRZ12kwwdAklE3tnlOj3ftxf/V+62fVro9tmavvAZHhERSQKHNImIJEgmwS/iMeEREUmQBL8OxIRHRCRF+pq0UpUw4RERSRB7eEREJAlSTHicpUlERJLAHh4RkQRxlqYeeXl5lflN2GfOnKnkaIiIpMVIevnOcAmvf//+hro0EZHksYenR+Hh4Ya6NBGR5HHSChERUQ1VJSatFBUVYcmSJdixYwdSU1ORn5+vcfzRo0cGioyIqGaS4pBmlejhzZ07F4sXL8bAgQORlZWF4OBgDBgwAEZGRoiIiDB0eDXe9m1b0KtHF7Tz8sTgDwfgzOlkQ4dE1cybbRrjq5ixuHFgAfLOLkdf/5Yax60szLBk5oe4vn8+Hh1fjLNf/wtjPuyoUcfM1ASLZ36I24cW4kHS5/hPzFg0qFdbj3chLUay8m/VVZVIeFu2bMHatWsxY8YMmJiYYMiQIVi3bh3mzJmDEydOGDq8Gm3/d/sQvTAKYz4ej+1f7UKbNt6YMHYM0u7dM3RoVI1YWchx4dpdTFu4o9Tj0TPeR3dfD4yatQmtB3yKZVsOY3Hoh+jj76mu8++Q99Gvc0t8FBaLrqOWoJaFGb5eOg5G1fkvbBUmq8A/1VWVSHjp6enw9Hz2i1+rVi1kZWUBAPr06YO9e/caMrQaL35jLN57/30M+OBDuDVujNCwWXCs74gd27cZOjSqRg78fBlzV36Lbw6dL/W4T0tXbP72F/x0+nekpj3Chp0/49drd9HGwxkAYFPLHCP7d8A/Fyfi8C9Xcf7qHYz+1ya0aKJEF5/X9XkrkiGTlX+rrqpEwmvYsCHS0tIAAE2aNMGBAwcAAKdOnYJcLjdkaDVaQX4+rly+hA6+mkNLHXzfxPlzZw0UFdVESeduoI+fJ5R1FQCAt9q6w92lHr5PugIA8GrmDDNTE3x//Iq6Tdr9LFz64x7eaOVqkJhrOlkFtuqqSkxaee+99/DDDz/Ax8cHU6dOxZAhQ7B+/XqkpqZi2rRphg6vxsp8nImioiLY2dlplNvZ2ePBg/sGiopqoumL/oOVc4bijwMLUFBQhGJRjPHztiLp3A0AgKOdDVT5BXj8Z55Gu4yHf8LBzsYQIVMNVCUS3sKFC9X//sEHH6Bhw4ZISkpCkyZN0K9fv1e2ValUUKlUGmXCWM6eoRZefOONEKLMb8EhKouJQ/zR3rMR3p+6Gqlpj9CxTRN8ETYI6Q+ycfiXqy9tJ5PJIPQYp5QYSfB/41ViSPNFb7zxBoKDg/822QFAVFQUFAqFxvbvRVF6iLL6q1O7DoyNjfHgwQON8kePHsLOzt5AUVFNYy43xdzJfTHz853Yd/QiLv5+D6u3H8VXB84gaERXAED6w2zIzUxR29pCo21d21rIeJhtiLBrPCkOaVaZhBcfH48333wTSqUSt27dAgDExMTgm2++eWW7sLAwZGVlaWwhM8P0EXK1Z2pmhmYezXEi6WeN8hNJSWjV2stAUVFNY2piDDNTExQLzb5aUVGxegbm2SupyC8oRNc3/m+CiqO9DZo3VuLE+RS9xisZEsx4VSLhrVq1CsHBwXjnnXfw+PFjFBUVAQBq166NmJiYV7aVy+WwsbHR2DicWXYjAkZh59dfIXHnV7jxxx/498JIpKWl4cNBgw0dGlUjVhZmaNm0AVo2bQAAaNTADi2bNoCTYx38mfsUR5N/R2RQf3TydoeL0g7D+/pgWJ/22H342azO7JyniNt1HAuDB8C/fVO0eq0hNnwagIvX7+HQL78Z8tZqLCkuS5AJIQw+RO7h4YHIyEj0798f1tbWOH/+PNzc3HDx4kX4+/uXGHL7O08LKynQGmr7ti2I27Ae9+9noIl7U4TMDIN323aGDqtaqNNukqFDqBI6ebvjwLqpJcrjd5/Ax+Gb4WBnjXmT30W3Dq+jjo3l/1+akISlmw+p68rNTBA17T0MfLstLOSmOHzyKoKituPO/x7r8U6qrryzy3V6vpM3ssrdtr2bQoeR6E+VSHgWFhb47bff4OLiopHwfv/9d7Rs2RJ5eXl/f5K/YMIjfWHCI32pCQkvKioKn3zyCaZOnaoevRNCYO7cuVizZg0yMzPh4+ODFStWoHnz5uWO72WqxJCmq6srzp07V6L8u+++Q7NmzfQfEBFRDafvR3inTp3CmjVr0LKl5mvnoqOjsXjxYixfvhynTp2Co6Mjunfvjj///LOcV3q5KpHwQkJCMHHiRGzfvh1CCJw8eRILFixAWFgYQkNDDR0eEVHNo8eMl5OTg2HDhmHt2rWoU6eOulwIgZiYGMyaNQsDBgxAixYtsHHjRjx58gRbt26t0O2Vpkqswxs1ahQKCwsRGhqKJ0+eYOjQoWjQoAGWLVuGTp06GTo8IqIapyKTT0pb/yyXv3z988SJE9G7d29069YNn376qbo8JSUF6enp6NGjh8Z5/Pz8kJSUhLFjx5Y7xtJUiR4eAIwZMwa3bt1CRkYG0tPTcfLkSZw9exZNmjQxdGhERDVORd6lWdr656io0tc/JyQk4MyZM6UeT09PBwA4ODholDs4OKiP6ZJBE97jx48xbNgw1K1bF0qlEkuXLoWtrS1WrFiBJk2a4MSJE9iwYYMhQyQiqpEqMqJZ2vrnsLCS659v376NqVOnYvPmzTA3N395LHp625NBhzQ/+eQTHD16FAEBAdi/fz+mTZuG/fv34+nTp9i3bx/8/PwMGR4REZXiVcOXf3X69GlkZGTA29tbXVZUVISjR49i+fLluHr12Wvl0tPTUb9+fXWdjIyMEr0+XTBoD2/v3r2IjY3FZ599ht27d0MIgaZNm+LQoUNMdkRElUkPk1a6du2KCxcu4Ny5c+qtbdu2GDZsGM6dOwc3Nzc4Ojri4MGD6jb5+fk4cuQIfH19dXKbf2XQHt69e/fg4eEBAHBzc4O5uTn+8Y9/GDIkIiJJ0McbU6ytrdGiRQuNMisrK9jZ2anLg4KCEBkZCXd3d7i7uyMyMhKWlpYYOnSozuMxaMIrLi6Gqampet/Y2BhWVlYGjIiISBqqyscSQkNDkZeXhwkTJqgXnh84cADW1tY6v5ZB37RiZGSEXr16qceC9+zZgy5dupRIejt37tTqvHzTCukL37RC+qLrN62cTy3/wu5WzrpPRvpg0B5eQECAxv7w4cMNFAkRkcRUkR6ePhk04cXGxhry8kREJCFV4k0rRESkX9X5Mz/lxYRHRCRBVWXSij4x4RERSZAE8x0THhGRJEkw4zHhERFJkBSf4VWZryUQERFVJvbwiIgkiJNWiIhIEiSY75jwiIgkSYIZjwmPiEiCpDhphQmPiEiCpPgMj7M0iYhIEtjDIyKSIAl28JjwiIgkSYIZjwmPiEiCOGmFiIgkQYqTVpjwiIgkSIL5jrM0iYhIGtjDIyKSIgl28ZjwiIgkiJNWiIhIEjhphYiIJEGC+Y6TVoiIJElWgU0LUVFRaNeuHaytrVGvXj30798fV69e1agjhEBERASUSiUsLCzg7++PS5cuVej2SsOER0RElebIkSOYOHEiTpw4gYMHD6KwsBA9evRAbm6uuk50dDQWL16M5cuX49SpU3B0dET37t3x559/6jQWmRBC6PSMVcDTQkNHQFJRp90kQ4dAEpF3drlOz3froarcbV3s5OVue//+fdSrVw9HjhzBW2+9BSEElEolgoKCMHPmTACASqWCg4MDFi1ahLFjx5b7Wi9iD4+ISIJksvJvKpUK2dnZGptKVbYEmpWVBQCwtbUFAKSkpCA9PR09evRQ15HL5fDz80NSUpJO75kJj4hIgiryCC8qKgoKhUJji4qK+ttrCiEQHByMjh07okWLFgCA9PR0AICDg4NGXQcHB/UxXeEsTSIiCarIsoSwsDAEBwdrlMnlfz/MOWnSJPz66684duxYKfFoBiSEKFFWUUx4RESSVP5kIpeblSnB/dXkyZOxe/duHD16FA0bNlSXOzo6AnjW06tfv766PCMjo0Svr6I4pElERJVGCIFJkyZh586dOHToEFxdXTWOu7q6wtHREQcPHlSX5efn48iRI/D19dVpLOzhERFJkL7etDJx4kRs3boV33zzDaytrdXP5RQKBSwsLCCTyRAUFITIyEi4u7vD3d0dkZGRsLS0xNChQ3UaCxMeEZEE6etNK6tWrQIA+Pv7a5THxsZi5MiRAIDQ0FDk5eVhwoQJyMzMhI+PDw4cOABra2udxsJ1eEQVwHV4pC+6XoeXlpVf7rb1FWY6jER/2MMjIpIgfi2BiIikQXr5jrM0iYhIGtjDIyKSIAl28JjwiIikiB+AJSIiSeCkFSIikgbp5TsmPCIiKZJgvuMsTSIikgb28IiIJIiTVoiISBI4aYWIiCRBij08PsMjIiJJYA+PiEiC2MMjIiKqodjDIyKSIE5aISIiSZDikCYTHhGRBEkw3zHhERFJkgQzHietEBGRJLCHR0QkQZy0QkREksBJK0REJAkSzHd8hkdEJEmyCmzlsHLlSri6usLc3Bze3t746aefKnoHWmPCIyKSIFkF/tHW9u3bERQUhFmzZuHs2bPo1KkTevXqhdTU1Eq4s5eTCSGEXq+oB08LDR0BSUWddpMMHQJJRN7Z5bo9X0H521qYalffx8cHbdq0wapVq9RlzZo1Q//+/REVFVX+QLTEZ3hERBJUkUkrKpUKKpVKo0wul0Mul5eom5+fj9OnT+Of//ynRnmPHj2QlJRU/iDKoUYmPPMaeVeVS6VSISoqCmFhYaX+0lLpdP3/uqWAv2tVQ0X+TkZ8GoW5c+dqlIWHhyMiIqJE3QcPHqCoqAgODg4a5Q4ODkhPTy9/EOVQI4c0SXvZ2dlQKBTIysqCjY2NocOhGoy/a9WfNj28e/fuoUGDBkhKSkKHDh3U5QsWLEB8fDx+++23So/3OfaFiIhIKy9LbqWxt7eHsbFxid5cRkZGiV5fZeMsTSIiqjRmZmbw9vbGwYMHNcoPHjwIX19fvcbCHh4REVWq4OBgjBgxAm3btkWHDh2wZs0apKamYty4cXqNgwmPADwboggPD+ckAqp0/F2TnkGDBuHhw4eYN28e0tLS0KJFC+zbtw8uLi56jYOTVoiISBL4DI+IiCSBCY+IiCSBCY+IiCSBCY8AAHFxcahdu7ahwyAqISIiAq1btzZ0GFQDMOHVMCNHjoRMJiuxXb9+3dChUQ301983ExMTODs7Y/z48cjMzDR0aEQlcFlCDfT2228jNjZWo6xu3boGioZquue/b4WFhbh8+TJGjx6Nx48fY9u2bYYOjUgDe3g1kFwuh6Ojo8b2xRdfwNPTE1ZWVnBycsKECROQk5Pz0nM8fPgQ7du3R79+/fD06VMIIRAdHQ03NzdYWFigVatW+Oqrr/R4V1RVPf99a9iwIXr06IFBgwbhwIED6uOxsbFo1qwZzM3N8frrr2PlypUa7WfOnImmTZvC0tISbm5umD17NgoKKvDtGqKXYA9PIoyMjLB06VI0atQIKSkpmDBhAkJDQ0v88QGAO3fuoEePHmjbti02bNgAExMTzJo1Czt37sSqVavg7u6Oo0ePYvjw4ahbty78/PwMcEdUFd24cQP79++HqemzD6atXbsW4eHhWL58Oby8vHD27FmMGTMGVlZWCAgIAABYW1sjLi4OSqUSFy5cwJgxY2BtbY3Q0FBD3grVRIJqlICAAGFsbCysrKzU2wcffFCi3o4dO4SdnZ16PzY2VigUCnH16lXh7OwsJk+eLIqLi4UQQuTk5Ahzc3ORlJSkcY7AwEAxZMiQyr0hqtL++vtmbm4uAAgAYvHixUIIIZycnMTWrVs12syfP1906NDhpeeMjo4W3t7e6v3w8HDRqlWrSomfpIU9vBqoc+fOGl8WtrKywuHDhxEZGYnLly8jOzsbhYWFePr0KXJzc2FlZQUAyMvLQ8eOHTFkyBB88cUX6vaXL1/G06dP0b17d43r5Ofnw8vLSz83RVXW89+3J0+eYN26dbh27RomT56M+/fv4/bt2wgMDMSYMWPU9QsLC6FQKNT7X331FWJiYnD9+nXk5OSgsLCQnw2iSsGEVwNZWVmhSZMm6v1bt27hnXfewbhx4zB//nzY2tri2LFjCAwM1HhWIpfL0a1bN+zduxchISFo2LAhAKC4uBgAsHfvXjRo0EDjWnwfIv31923p0qXo3Lkz5s6di0mTJgF4Nqzp4+Oj0cbY2BgAcOLECQwePBhz585Fz549oVAokJCQgM8//1y/N0GSwIQnAcnJySgsLMTnn38OI6Nn85R27NhRop6RkRHi4+MxdOhQdOnSBT/++COUSiU8PDwgl8uRmprK53X0t8LDw9GrVy+MHz8eDRo0wI0bNzBs2LBS6/78889wcXHBrFmz1GW3bt3SV6gkMUx4EtC4cWMUFhZi2bJl6Nu3L37++WesXr261LrGxsbYsmULhgwZok56jo6OmDFjBqZNm4bi4mJ07NgR2dnZSEpKQq1atdSTD4gAwN/fH82bN0dkZCQiIiIwZcoU2NjYoFevXlCpVEhOTkZmZiaCg4PRpEkTpKamIiEhAe3atcPevXuRmJho6FugGorLEiSgdevWWLx4MRYtWoQWLVpgy5YtiIqKeml9ExMTbNu2Dc2bN0eXLl2QkZGB+fPnY86cOYiKikKzZs3Qs2dP7NmzB66urnq8E6ougoODsXbtWvTs2RPr1q1DXFwcPD094efnh7i4OPXvzbvvvotp06Zh0qRJaN26NZKSkjB79mwDR081FT8PREREksAeHhERSQITHhERSQITHhERSQITHhERSQITHhERSQITHhERSQITHhERSQITHhERSQITHtVYERERaN26tXp/5MiR6N+/v97juHnzJmQyGc6dO1dp13jxXstDH3ESGRITHunVyJEjIZPJIJPJYGpqCjc3N8yYMQO5ubmVfu0vvvgCcXFxZaqr7z/+/v7+CAoK0su1iKSKL48mvXv77bcRGxuLgoIC/PTTT/jHP/6B3NxcjW/4PVdQUKD+enZF/fUbbEQkPezhkd7J5XI4OjrCyckJQ4cOxbBhw7Br1y4A/zc0t2HDBri5uUEul0MIgaysLHz88ceoV68ebGxs0KVLF5w/f17jvAsXLoSDgwOsra0RGBiIp0+fahx/cUizuLgYixYtQpMmTSCXy+Hs7IwFCxYAgPrlxl5eXpDJZPD391e3i42NRbNmzWBubo7XX38dK1eu1LjOyZMn4eXlBXNzc7Rt2xZnz56t8M9s5syZaNq0KSwtLeHm5obZs2drfMvwuS+//BJOTk6wtLTEhx9+iMePH2sc/7vYiWoy9vDI4CwsLDT+eF+/fh07duzA119/rf5QaO/evWFra4t9+/ZBoVDgyy+/RNeuXXHt2jXY2tpix44dCA8Px4oVK9CpUyfEx8dj6dKlcHNze+l1w8LCsHbtWixZsgQdO3ZEWloafvvtNwDPklb79u3x/fffo3nz5jAzMwPw7GOm4eHhWL58Oby8vHD27FmMGTMGVlZWCAgIQG5uLvr06YMuXbpg8+bNSElJwdSpUyv8M7K2tkZcXByUSiUuXLiAMWPGwNraGqGhoSV+bnv27EF2djYCAwMxceJEbNmypUyxE9V4gkiPAgICxLvvvqve/+WXX4SdnZ0YOHCgEEKI8PBwYWpqKjIyMtR1fvjhB2FjYyOePn2qca7GjRuLL7/8UgghRIcOHcS4ceM0jvv4+IhWrVqVeu3s7Gwhl8vF2rVrS40zJSVFABBnz57VKHdychJbt27VKJs/f77o0KGDEEKIL7/8Utja2orc3Fz18VWrVpV6rr/y8/MTU6dOfenxF0VHRwtvb2/1fnh4uDA2Nha3b99Wl3333XfCyMhIpKWllSn2l90zUU3BHh7p3bfffotatWqhsLAQBQUFePfdd7Fs2TL1cRcXF9StW1e9f/r0aeTk5MDOzk7jPHl5efjjjz8AAFeuXMG4ceM0jnfo0AGHDx8uNYYrV65ApVKha9euZY77/v37uH37NgIDAzFmzBh1eWFhofr54JUrV9CqVStYWlpqxFFRX331FWJiYnD9+nXk5OSgsLAQNjY2GnWcnZ3RsGFDjesWFxfj6tWrMDY2/tvYiWo6JjzSu86dO2PVqlUwNTWFUqksMSnFyspKY7+4uBj169fHjz/+WOJctWvXLlcMFhYWWrcpLi4G8Gxo0MfHR+PY86FXUQmflzxx4gQGDx6MuXPnomfPnlAoFEhISMDnn3/+ynYymUz9n2WJnaimY8IjvbOyskKTJk3KXL9NmzZIT0+HiYkJGjVqVGqdZs2a4cSJE/joo4/UZSdOnHjpOd3d3WFhYYEffvgB//jHP0ocf/7MrqioSF3m4OCABg0a4MaNGxg2bFip5/Xw8EB8fDzy8vLUSfVVcZTFzz//DBcXF8yaNUtdduvWrRL1UlNTce/ePSiVSgDA8ePHYWRkhKZNm5YpdqKajgmPqrxu3bqhQ4cO6N+/PxYtWoTXXnsN9+7dw759+9C/f3+0bdsWU6dORUBAANq2bYuOHTtiy5YtuHTp0ksnrZibm2PmzJkIDQ2FmZkZ3nzzTdy/fx+XLl1CYGAg6tWrBwsLC+zfvx8NGzaEubk5FAoFIiIiMGXKFNjY2KBXr15QqVRITk5GZmYmgoODMXToUMyaNQuBgYH417/+hZs3b+Kzzz4r033ev3+/xLo/R0dHNGnSBKmpqUhISEC7du2wd+9eJCYmlnpPAQEB+Oyzz5CdnY0pU6Zg4MCBcHR0BIC/jZ2oxjP0Q0SSlhcnrbwoPDxcY6LJc9nZ2WLy5MlCqVQKU1NT4eTkJIYNGyZSU1PVdRYsWCDs7e1FrVq1REBAgAgNDX3ppBUhhCgqKhKffvqpcHFxEaampsLZ2VlERkaqj69du1Y4OTkJIyMj4efnpy7fsmWLaN26tTAzMxN16tQRb731lti5c6f6+PHjx0WrVq2EmZmZaN26tfj666/LNGkFQIktPDxcCCFESEiIsLOzE7Vq1RKDBg0SS5YsEQqFosTPbeXKlUKpVApzc3MxYMAA8ejRI43rvCp2Tlqhmk4mRCU8dCAiIqpiuPCciIgkgQmPiIgkgQmPiIgkgQmPiIgkgQmPiIgkgQmPiIgkgQmPiIgkgQmPiIgkgQmPiIgkgQmPiIgkgQmPiIgk4f8B+GaipQxb1RIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Collect predictions and true labels\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "model.eval()  # Set to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "        outputs = model(images)\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Fake\", \"Real\"], yticklabels=[\"Fake\", \"Real\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd081d09-c2ec-4903-a370-8e566046527d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "def predict_image(image_path, model, device):\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to 3-channel\n",
    "    transforms.Resize((224, 224)),  # Resize to match ResNet input\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize for 3 channels\n",
    "])\n",
    "    \n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Keep it RGB (3 channels)\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        prediction = torch.round(torch.sigmoid(output)).item()\n",
    "    \n",
    "    return \"Real\" if prediction == 1 else \"Fake\"\n",
    "\n",
    "# Example usage\n",
    "print(predict_image(\"024_073_8_png.rf.6e2abf6d730a7cf1d0f0a17f715726f4.jpg\", model, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "175516e7-10b6-4315-8ce6-186813552629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 99.44%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "        outputs = model(images)\n",
    "        predictions = torch.round(torch.sigmoid(outputs))\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34367d00-00fa-4b8a-9255-21382d45dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"deepfake_detector.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64bd4f1d-37dd-4635-a229-dcafb5bde676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khadi\\AppData\\Local\\Temp\\ipykernel_24244\\2455722962.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"deepfake_detector.pth\", map_location=device))  # Load .pth file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.7647762298584\n",
      "Prediction: Real\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the trained model\n",
    "model = models.resnet50()\n",
    "model.fc = nn.Linear(model.fc.in_features, 1)  # Binary classification (Real/Fake)\n",
    "model.load_state_dict(torch.load(\"deepfake_detector.pth\", map_location=device))  # Load .pth file\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Define image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to 3-channel\n",
    "    transforms.Resize((224, 224)),  # Resize to match ResNet input\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize for 3 channels\n",
    "])\n",
    "\n",
    "# Function to predict an image\n",
    "def predict_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Ensure it's in RGB mode\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Apply transformations and add batch dimension\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image).item()  # Get raw output\n",
    "        print(output)\n",
    "    \n",
    "    prediction = \"Real\" if output >=0 else \"Fake\"\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "\n",
    "# Example usage\n",
    "predict_image(\"001_8_png.rf.767bd9d16f6aef233b8d3474082002a7.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231b293a-f697-406a-bdec-b25b35c30875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e5b125-d093-4c58-badf-9185ee1496b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd26db3-4fd6-467d-89c9-9f943ee49383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pre-trained ResNet50\n",
    "model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Linear(model.fc.in_features, 1)  # Binary classification\n",
    "model = model.to(device)\n",
    "\n",
    "# Load trained weights (change path to your model)\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Define transform (must match training transform)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Match training\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "# Function to extract and classify frames\n",
    "def predict_video(video_path, frame_interval=10, max_frames=100):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    predictions = []\n",
    "\n",
    "    while cap.isOpened() and frame_count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_count % frame_interval == 0:\n",
    "            # Convert frame using same transform\n",
    "            img = transform(frame).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(img)\n",
    "                prob = torch.sigmoid(output).item()\n",
    "                predictions.append(prob)\n",
    "                print(f\"Frame {frame_count}: {'Fake' if prob > 0.5 else 'Real'} ({prob:.2f})\")\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if predictions:\n",
    "        avg_score = sum(predictions) / len(predictions)\n",
    "        final = \"Fake\" if avg_score > 0.5 else \"Real\"\n",
    "        print(f\"\\nFinal Verdict: {final} (Avg Score: {avg_score:.2f})\")\n",
    "    else:\n",
    "        print(\"No frames processed.\")\n",
    "\n",
    "# Run on your video (change path)\n",
    "predict_video(\"input_video.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f18dad-a9d2-47ab-ac82-6524f02692a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eef019-6edb-4e82-a58c-4aa4ffa38aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128158bc-0ef6-4e8b-b174-4356c47fbfa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b6cad-8e33-44ab-9650-94e2bc78b0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
